# -*- coding: utf-8 -*-
"""Beginner01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aZXYhcx9ADg2SpOthKEJtIxYP4yA28Tf
"""

from google.colab import files
uploaded = files.upload()

#This time pandas is forbidden, you have to use only numpy
import numpy as np

# Read the data from the CSV file with read from Numpy
data = np.genfromtxt("Salary_dataset.csv", delimiter=',')
print("data: ", data)

# Separate the features (YearsExperience) from the target variable (Salary)
X = np.array(data)[1:, 1]
y = np.array(data)[1:, 2]
print("x: ", X)
print("y: ", y)

#This time seaborn is forbidden, you have to find a library that works with numpy
import matplotlib.pyplot as plt

#You have to reproduce this graph
plt.plot(X, y, 'bo',)

# Let's create a function that displays the point line with the bar.
def visualize(theta, X, y):
  plt.figure()
  y_line = theta[0] + theta[1] * X
  plt.plot(X, y, 'bo', X, y_line, 'b-')

# Ok, let's test our function now, you should get a result comparable to this one
theta = np.zeros(2)
visualize(theta, X, y)

# Create a function that multiplies each element of the matrix X by the slope of the model (theta[1]),
#followed by the addition of the intercept of the model (theta[0]), thus producing the predictions of the simple linear regression model.
def predict(X, theta):
    return theta[0] + theta[1] * X

def fit(X, y, theta, alpha, num_iters):
    # Initialize some useful variables
    m = X.shape[0]

    # Loop over the number of iterations
    for _ in range(num_iters):
        # Perform one iteration of gradient descent (i.e., update theta once)
        y_line = predict(X, theta)
        diff = y_line - y
        gradient = np.array([
            np.sum(diff)/m,                  # theta[0] - height
            np.sum(diff * X)/m               # theta[1] - slope
        ])
        theta = theta - (alpha * gradient)
    return theta

# To begin, we'll set alpha to 0.01 and num_iters to 1000

theta = np.zeros(2)
finetuned_theta = fit(X, y, theta, 0.01, 1000)
print(finetuned_theta)

#You should have a result similar to this one: [21912.58918422329, 9880.814004608217]

# Ok, let's test our function now, you should get a result comparable to this one
theta = np.zeros(2)
visualize(fit(X, y, theta, 0.01, 0), X, y)
visualize(fit(X, y, theta, 0.01, 1), X, y)
visualize(fit(X, y, theta, 0.01, 2), X, y)
visualize(fit(X, y, theta, 0.01, 3), X, y)
visualize(fit(X, y, theta, 0.01, 4), X, y)
visualize(fit(X, y, theta, 0.01, 1000), X, y)

def cost(X, y, theta):
    # number of samples
    m = len(y)

    # Calculate the difference between model predictions and actual target values
    y_line = predict(X, theta)
    diff = y - y_line

    # Calculate the squared sum of the loss and scale it by 1/(2 * number of samples)
    cost = np.sum(diff**2)/(2*m) # math function to calculate cost

    # Return the computed cost as a measure of model fit
    return cost

# Test it with theta = [0,0]. You should get approximately 3251553638.

cost_for_theta_zero = cost(X, y, [0, 0])
print(cost_for_theta_zero)

def fit_with_cost(X, y, theta, alpha, num_iters):
    m = X.shape[0]  # Number of training examples
    J_history = []  # List to store cost values at each iteration

    # Loop over the specified number of iterations
    for itr in range(num_iters):

        # Calculate the loss (difference between predictions and actual values)
        y_line = predict(X, theta)
        diff = y_line - y

        # Update the temporary values of theta for both coefficients using the gradient descent formula
        gradient = np.array([
            np.sum(diff)/m,                  # theta[0]
            np.sum(diff * X)/m               # theta[1]
        ])

        # Update the theta values
        theta = theta - (alpha * gradient)

        # Calculate the cost for the current theta values
        current_cost = cost(X, y, theta)

        # Append the cost to the history list
        J_history.append(current_cost)

    # Return the final theta values and the list of cost values over iterations
    return (theta, J_history)

# First, we initialize theta to zero
theta = np.zeros(2)

# Start the training using your new function
theta, J_history = fit_with_cost(X, y, theta, 0.001, 100)
plt.figure()
plt.plot(J_history)
#visualize(fit(X, y, theta, 0.01, 100), X, y)

#You have to reproduce this graph

# Years of experience of the person you want to predict the salary for
years_experience = 10

# Predict the salary
predicted_salary = predict(years_experience, theta)

# Display the predicted salary
print("Predicted salary for {} years of experience {}".format(years_experience, predicted_salary))